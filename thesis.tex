\documentclass[12pt,oneside,draft]{fithesis2}
\usepackage[english]{babel} % package for multilingual support
\usepackage[utf8]{inputenc} % Windows OS encoding
\usepackage[T1]{fontenc}
\usepackage[plainpages=false,pdfpagelabels,unicode]{hyperref}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{enumerate}

\newtheorem{defn}{Definition}

\thesistitle{Evolutionary optimization of intrusion detection system in wireless sensor networks} % enter thesis title
\thesissubtitle{Diploma thesis}
\thesisstudent{Adam Saleh}
% name of the author
\thesiswoman{false}
% defines author’s gender
\thesisfaculty{fi}
\thesisyear{spring 2008}
\thesisadvisor{Andryi Stetsko, Ph.D.} % fill in advisor’s name
\thesislang{en}
% thesis is in English
\begin{document}
\FrontMatter
\ThesisTitlePage
\begin{ThesisDeclaration}
\DeclarationText
\AdvisorName
\end{ThesisDeclaration}
\begin{ThesisThanks}
I would like to thank my supervisor ...
\end{ThesisThanks}
\begin{ThesisAbstract}
\end{ThesisAbstract}
Wireless Sensor networks are an interesting topic for security research because of their possible real world applications, their distributed nature and the constrains on their hardware. For example, setting up intrusion detection system on network of nodes so that it satisfies constrains on accuracy and hardware can be a timeconsuming.
We believe that this process could be greatly improved by using genetic algorithms.
Purpouse  of this work is to provide a reasonably reusable framework for optimizations with evolutionary heuristics and calibrate it for optimizing of Intrusion Detection System on Wireless Sensor Network. 
\begin{ThesisKeyWords}
Wsn,ids,spea2,nsgaii
\end{ThesisKeyWords}

\tableofcontents % prints table of contents

\MainMatter
\chapter{Introduction}


\chapter{Evolutionary Optimization of Wireles Sensor Networks}
Optimizing configuration of a wireless sensor network can be a time-consuming task. Nodes of the network are constrained devices,
so tradeoffs between battery life, network, memory and processor usage have to be considered. Because it is difficult to to predict how will different configuration settings change the performance of the network, it is advisable to measure the impact of changing configuration with WSN simulation software. Unfortunately, simulation of such configuration can take anywhere between several minutes to several hours of processor time, rendering usage of simple exhaustive search on all configurations impractical for most WSN calibration purposes.

According to (b Talbi), optimization problems with problem space that is hard to characterize and expensive to search are good fit for heuristic approaches.

[copy figure 1.9 from talbi]


\begin{defn}[Optimization problem]
We define optimization problem as a triplet $(X,Z,f)$,
 where $X \subseteq \mathbb{R}^n$ is the decision space,
$Z \subseteq \mathbb{R}^m$ is the objective space and $f:X \to Z$ is the evaluation function.
Without loss of generality it is assumed, that we want to minimize objective.
\end{defn}

Because we focused on multi-objective optimization, we used definition of optimization problem from (c IBEA), mainly to maintain constistency in other definitions in later chapters. This defintions allows the evaluation function $f$ to output incomparable objective vectors, which is a key concept for multi-criteria optimization. Optimization problem can be easily made compatible with single-criteria algorithms by defining cost-function $c:Z \to \mathbb{R}$, that creates an agregatted metric from the objective vector.

\section{Basic principles of metaheuristics}

Most of the heuristics used are variants on search through problem space, with different approaches to avoid undesirable local optima.
There are two basic principles that are used when designing a metaheuristic:

\subsection{Diversification:}
an algorithm should explore the search space.

Random search can be considered an extreme application of difersification, where in every round we generate next solution randomly without using any information from previous (good) solutions.

Less extreme example is the family of population based heuristics, where in each iteration we are trying to improve a set of candidates, called a population.

\subsection{Intensification:} an algorithm should exploit information from the best solutions found.

Local search can be considered an extreme application of intensification, where next instance to evaluate is allways selected deterministicaly, based on the previous solution.

Local search and its variants belong to a family of single-solution based heuristics, where in each iteration we are basing our search of the problem-space of single solution.
(b talbi)

Because of their nature, heuristic approaches are hard to evaluate and reason about, therefore they are often considered to be a method of last resort. (b Talbi) warns against using heuristics laizily on problems where more deterministic approaches (such as using a SAT-solver or linear programming) would be more efficient. Heuristic approaches are usually considered whe we need to solve our problem in order of magnitude faster than possible using conventional approaches, while sacrificing guarantees on optimality.

In our case, we are constrained by number of configuration evaluations we are able to perform in a given timeframe. 
We are focusing on evolutionary heuristics, because they seem to have reasonable performance with regard of optimizing WSN (b Stetsko 2011). 
These are modeled natural proces of evolution of species. It is a family os stochastic, population-based heuristics (as opposed to deterministic, single-solution based). 

\section{Single objective evolutionary algorithms}

Basic principles behind evolutionary algorithms can be shown on example of simple evolutionary algorithm. 

Evolutionary algorithms are based of the fact, that natural proces of evolution can be considered an algorithm solving an optimization problem of adapting a species to its environment. 

\subsection{Template of an evolutionary algorithm. (b Talbi)}

\begin{description}
	\item[Step 0: Initialization:] Generate an initial population of individuals
	\item[Step 1: Fitness assigment:] calculate the fittness values of individuals in population
	\item[Step 2: Enviromental selection:] select the best individuals of a population to "survive" to the next generation
	\item[Step 3: Termination:] check if termination criterium holds (usually based on number of generations), if it does, return the result and stop.
	\item[Step 4: Variation:] apply mutation and crossover parameters on the surviving individuals, increment the generation counter and proceed with step 1.
\end{description}

If we continue with this metaphor, we can find these paralels:
\begin{itemize}
\item Population of individuals is a set of configurations. Their environment is the optimization problem.
\item We decide surviving individuals based on their evaluation function score.
\item From surviving individuals we generate next generation, either by mutation or crossover.
\item Mutation of individual usualy consists of randomly selecting some configuration from its neghbourhood as its ofspring. This step provides intensification for evolution heuristic.
\item Crosover usualy consists of creating an offspring by permutation of two individuals. This step provides diversification for evolution heuristic.
\end{itemize}

In algorithms, there are variations on this template, for example, instead of first selecting the best and then generating the rest of population, \emph{variation} can precede \emph{enviromental selection}, and enviromental selection then trims the population to the confines of preset population limit.

In our framework selection and replacement steps are part of the evolutionary algorithm, while crossover and mutation functions are to be calibrated on case by case basis.

\section{Multiobjective evolution algorithms}
Problem that we are trying to optimize is unfortunately illsuited for single objective evolution. Objectives such as memory consumption and IDS accuracy are ortoghonal, therefore it is hard to determine single criteria that would satisfactory cover both of them. A usual solution is to provide some sort of weighted average. In this case,because output of our algorithm would be only a single solution, we need to know emphasis on different criteria before we run our optimization. 

Better approach is to recognize multi-objective nature of our problem.

Multi objective algorithms on the other hand are based on idea of Pareto optimality and domination. We say that solution A dominates solution B, if A has no objective "worse" than B and at least one objective "better". We say that A is pareto-optimal, if there is no other solution that would dominate A. This means that no objective of A can be improved without deterioration of another objective. Set of all non-dominated solutions is then called Pareto front. 

Subset of solutions where no solution from the superset dominate the one in the set is called Pareto optimal set.

We will call a subset that contains only solutions that don't dominate each other a Pareto aproximation.

\begin{defn}{Pareto dominance}
An objective vector $u = (u_1,\cdots,u_n)$ is said to dominate $v = (v_1,\cdots,v_n)$ (denoted by $u \prec v$) if and only if no component of $v$ is smaller\footnote{we assume minimization} than the coresponding component of $u$ and at least one component of $u$ is strictly smaller:

$$ \forall i \in \langle 1,n\rangle: u_i \leq v_i \land \exists i \in \langle 1,n\rangle: u_i < v_i $$
\end{defn}

\begin{defn}{Pareto optimality}
A solution $x \in S$ is Pareto optimal if for every $x' \in S$, $F(x')$ does not dominate $F(x)$, that is $$ \forall x' \in S, F(x) \not \prec F(x') $$
\end{defn}

\begin{defn}{Pareto optimal set}
  For given MOP(F,S), the pareto optimal set is defined as $\cal
P* = {x \in S | \not \exists x' \in S, F(x') \prec F(x)}$
\end{defn}

\begin{defn}{Pareto front}
  For given MOP(F,S), the pareto optimal set is defined as $\cal
P* = {x \in S | \not \exists x' \in S, F(x') \prec F(x)} $
\end{defn}

\begin{defn}{Pareto aproximation}
  For given MOP(F,S), we call set $A \subset S$ a  pareto aproximation if $\forall x \in A \not \exists x' \in A, F(x') \prec F(x)$
\end{defn}

Multiobjective heuristics then try to obtain best aproximation of Pareto front. To gauge how good an approximation of Pareto front is, we usualy use two criteria, first to measure convergence to Pareto optimal front and second to measure diversity in found pareto set.

[image about approximations]

Calibration of multi objective evolution algorithms is harder than their single criteria counter-parts, mainly because it is hard to reason about the convergence of output without having the real Pareto optimal front in the first place. Pareto optimal sets often aren't directly comparable. TODO We will discuss calibration in greater detail in later chapters.


\subsection{NsgaII}
In Non-dominated Sorting Genetic Algorithm, the Replace step is performed in two steps
\begin{enumerate}
\item first by sorting union of old $P(t)$ and new $P'(t)$ in into ranks based on the ordering of pareto dominance.
\item then sorting among members of each rank is performed based on similarity beween individuals. Best individuals are then chosen for the next iteration.
\end{enumerate}

\subsubsection{Non-dominated sorting}



\subsubsection{Crowding distance sorting}

We can see that first sorting helps with general convergence to real Pareto front, while second improves diversity in the nondominated part of the population. To get the result we just need to extract non-dominated results from the final population. 

\subsection{Spea2}
In Strength Pareto Evolutionary Algorithm, unlike previous NSGAii, stores non-dominated indivituals in archive separate from the rest of the population. Therefore after new population is generated, new contents of archive are determined in two passes, first using procedure to ensure convergence and then procedure to ensure diversity.

Let us denote:

$P_t$ to be the population in generation $t$,$\overline{P_t}$ the coresponding archive of nondominated individuals, $N$ the population size,$\overline N$ the archive size and $T$ the maximum number of generations.

The overall algorithm then is as follows: (cite SPEA)

\begin{description}
	\item[Step 1: Initialization:] Generate an initial population $P_0$ and create the empty archive $\overline{P_0}$, set $t=0$
	\item[Step 2: Fitness assigment:] calculate the fittness values of individuals in $P_t \cup \overline{P_t}$
	\item[Step 3: Enviromental selection:] Copy all nondominated individuals in $P_t \cup \overline{P_t}$ to $\overline{P_{t+1}}$. If size of $\overline{P_{t+1}}$ exceeds $\overline N$, then remove exceeding individuals with worst fitness, otherwise, if size of $\overline{P_{t+1}}$ is less than $\overline N$, fill $\overline{P_{t+1}}$ with dominated individuals in  with the best fitness $P_t \cup \overline{P_t}$
	\item[Step 3: Termination:] if $t \geq T$ or another stopping criterion is satisfied, then stop and return the result as the nondominated individuals from $\overline{P_{t+1}}$
	\item[Step 5: Mating selection:] generate the mating pool $M$
	\item[Step 6: Variation:] apply crossover and mutation operators to the mating pool and set $P_{t+1}$ to the resulting population. Increment generation counter and go to Step 1.
\end{description}

The final result then contains just the nondominated individuals of the archive.

\subsubsection{Fitness assigment}
	Fitness of an individual is an agregated metric composed of strength of individual and density estimation. Strength of an individual $i$ is the number of individuals from $P_t \cup \overline{P_t}$ it dominates:

$$ S(i) = |\{j| j \in P_t \cup \overline{P_t} \land i \prec j\}| $$

Raw fitness of $i$ is then agregated from all strengths of individuals that dominate $i$:

$$ R(i) = \sum\limits_{j \in P_t \cup \overline{P_t},j \prec i} S(j)$$

To distinguish between individuals that do not dominate each other a density metric based on the distance of $k$-th nearest neighbour of $i$, denoted $\delta^k_i$. As a common setting, $k$ equal to the square root of the sample size is used (cite spea2). Thus density is defined by:

$$D(i) = \frac{1}{\delta^k_i + 2} $$

Because of the two added in the denominator  $0 < D(i) < 1$. Agregate fitness of an individual is then the sum of its raw fitness and density:

$$F(i) = R(i) + D(i) $$

Because the raw fitness has integer values, sorting individuals by the fitness metric yelds similar results to the two pass sorting of NSGAii.

\subsubsection{Enviromental selection}
Enviromental selecton step depends on the number of nondominated individuals in the union. This is the set of individuals with fitness lower than one:

$$P'_{t+1} =\{i | i \in P_t \cup \overline{P_t} \land F(i)<1\}$$

If $|P'_{t+1}| < \overline N$ it is sufficient to set $\overline{P_t+1}$ to contain the best $\overline N$ individuals from $P_t \cup \overline{P_t}$ based on the fitness.

If $|P'_{t+1}| > \overline N$ a trimming procedure is used that is based on the density in the $P'_{t+1}$ set, as opposed to $P_t \cup \overline{P_t}$ used for fitness metric.

Iteratively, individual that has minimum distance to another individual is chosen to be removed. If there are several individuals with minimum distance, tie is broken by considering their second smallest distances and so forth. 

\subsection{Ibea}
Both NsgaII and Spea2 differ mostly in their aproach to characterize the convergence to Pareto front and diversity. 
Indicator-Based evolutionary algorithm, takes more abstract approach, based on a concept of binary quality indicators.
Indicator is a function, that takes two pareto sets of the same domain and outputs some quantification of a difference between the two.

Zitzler and Kunzli proposed two indicators,
\begin{enumerate}
\item additive $\epsilon$-indicator that quantifies the minimal distance first pareto set has to be moved in each dimension in objective space such that the second pareto set is weakly dominated.

\item hypervolume-distance indicator, that quantifies how much volume is dominated by the first pareto set but not dominated by the second, with respect to predefined reference point $Z$.
\end{enumerate}

Both of these indicators are dominance preserving. 

\begin{defn}
A binary quality indicator is denoted as dominance preserving if $\forall x_1,x_2,x_3 \in Z$:
\begin{enumerate}[(i)]
\item  $x_1 \prec x_2 \Rightarrow I({x_1},{x_2}) < I({x_2},{x_1})$, and
\item $x_1 \prec x_2 \Rightarrow I({x_3},{x_1}) > I({x_3},{x_2})$
\end{enumerate}
\end{defn}
Because any single individual can be considered a pareto set of size one, IBEA uses given indicator to quantify fitness of an individual.



\subsection{Choosing the algorithm}
Design of both of these algorithms give some guarantees on convergence to Pareto front. Both of them are incremental improvements on their predecessor, and while Spea2 is newer, it doesn't mean it is better in every instance. It has to be noted, that even comparison between these two algorithms with regards to optimizing IDS for WSN (Stehlik 2013) came out inconclusive. 

Of the more interesting concepts that IBEA uses is the hyper-volume distance indicator, which might provide an interesting alternative to more popular NSGA and SPEA, if provided with good reference point. We will tallk more about hyper-volume as a metric of pareto set convergence in chapter on calibration.

We would advise to focus on calibration only one algorithm.

\chapter{Our Framework}

(b Stetsko 2011) showed, that using simple evolutionary algorithms is a viable and efficient way of optimizing Wireless Sensor networks. Unfortunately, prevoius experimental setup was too tightly coupled with optimization framework, rendering the code of previous experiments hard to reuse. Therefore we have created a micro-framework based on python and ParadisEO. 

ParadisEO is a highly configurable framework for metaheuristics written in C++. Our micro-framework simplifies its usage by allowing to specify evaluation, mutation and crossover functions in python. Emphasis is on multiobjective algorithms, ability to evaluate population in parallel and having good facilities for experiment analysis.

We chose python, for these reasons:
\begin{enumerate}
\item it is an efficient glue language. 
Most often you will want to optimize already written application with minimal changes. Python is well suited for running external applications, specifying their inputs and parsing their outputs.
 
\item it has good facilities for statistical analysis.
That allows us to include analysis of every optimization run into the executable itself. Having automatically generated experimental log has proven invaluable especialy in callibration of algorithms for our spepcific problem.

\item it is easily integrated with C++.
We strive to make the integration with ParadisEO simple and extensible. 
\end{enumerate}
\section{Evaluation function}

To simplify configuration, inputs and outputs of evaluation function are allways a list of floating point numbers. We believe this is general enough, and that most WSN optimisation problems can be fitted to this constraint. Only problem might pose translation of combinatorial problems to continuous ones, fortunately so far we have been quite successfull with using just a simple rounding techniques. We wanted to avoid usage of binary strings as input vectors.

Many evolutionary algorithms support them and if we were using them we wouldn't need to concern ourselves with implementation details of mutation and crossover functions. On the other hand, with generalized mutation functions on binary strings, there are no guarantees that mutated individual will even be valid. CITATION In our opinion, vectors of reals are esier to reason about, and their convergence may provide valuable insight to structure of the problem.

\subsection{Bounds}
For inputs you specify their bounds, which are then used to initialize the population and to supply constraints for mutation and crossover parameters

[Example]

\subsection{Objectives}

Because we focus on multiobjective evolution, you need to specify number of objectives. To simplify configuration, all the parameters will be minimized.

[example]

It is advisable to have three objectives at maximum, because number of solutions on the Pareto optimal dramaticaly increase with new objectives.
At minimum, $(n+1)$ objective problem will contain all the solutions of an n-objective problem. [CITATION? 309 talbi]

Then there is the problem of visualizing and evaluating more than 3-dimensional data.

\subsection{Boinc Assisted Evolution}

Because evolution is population-based heuristic, it is well suited for parallel evaluation of its individuals. ParadisEO already has two methods included, either by use of MPI protocol [CITATION], or by using SMP on multi-core machine. Unfortunately, SMP module of ParadisEO is not yet stable on all platforms, and MPI has specific demands on infrastracture and is usually hard to retro-fit on already written program. 

Boinc on the other hand  can simply utilize machines already preasent to form a simple computational grid. With its simple archhitecture consisting of Management server and several worker nodes it can be deployed in most settings. Its simple architecture prohibits cooperation between worker nodes, but that doesn't concern us while evaluating individuals.

We have decided to give the evolution algorithms ability to use a simple scheduler for creating Boinc work units, plugable from python.

In our micro-framework it consists of three functions, schedule\_work\_unit, wait\_for\_completion and gather\_result.

Even though the evaluations themselves will be done in parallel, we wanted to avoid any threaded code in our framework itself. Because we have written it with research in mind, we understand that accessing auxiliary data is important to evaluate experiments. In threaded environment, this could lead to deadlocks.

\section{Calibration}

We can think of calibration on an evolutionary algorithm as a meta-optimization problem with two objectives,

\begin{enumerate}
\item maximizing the fittness of result
\item minimizing the number of evaluations
\end{enumerate}
With single-objective algorithms, optimizing for these two objectives is easy, because both of them are usualy represented by numbers.
With multi-objective problems, resuls are Pareto sets, which are often not directly comparable.

Quality of a multiobjective result can be judged based on several criteria:
\begin{enumerate}
\item number of results in Pareto set.
\item convergence tp true Pareto front
\item regularity in distance beween results
\end{enumerate}


\subsection{Exhaustive search}
Because the main reason we use multi-objective algorithms is to avoid the need to do the expensive computation of true pareto front,
we could calibrate it on a easier variant of the problem. Underlying assumption is, that the particular variant of the problem will have similar solution space. We will show such an exampe in following chapter.

Computing an exhaustive search for a subset of our problem can help in several ways:
\begin{itemize}
  \item we can check our assumptions about the shape and homogenity of solution space
  \item we can use the Pareto front as a reference point
  \item for any approximation, we can easily compute how many individuals it dominates and how many individuals dominate is
\end{itemize}

This is probably the most the most expensive, but on the other hand the most precise way to estimate, how will an evolutionary algorithm behave in a particluar problem domain.


\subsection{Iteratively}

For iterative improvements we have used different convergence critera. To measure convergence of Pareto aproximations to the Pareto optimal set, we can either specify a global reference point, with respect to which all approximations are measured, or we specify a comparison function that can provide ordering.

\subsubsection{Measuring convergence}

\paragraph{Hypervolume indicator}
One of the more popular is maximizing the hypervolume of the objective space dominated by the resulting approximation. Hypervolume indicator is based on idea of measuring the volume that a given individual dominates based on some reference point in the solution-space. This works with assumption that the solution-space is homogenous, compact, and that we can specify a reasonable reference point. A good reference point is usualy the worst individual [CITATION NEEDED] and because because we usualy don't know how will this individual look like, by guessing this reference point we are expresing certain biases (or expectations) on the shape of the solution space as well.

\paragraph{Distance indicator}
Distance indicator $I_{\epsilon^{+}}: Z \times Z \to  \mathbb{R}$ is one of possible choices for indicator that IBEA algorithm uses for comparing two pareto-approximations. $I_{\epsilon^{+}}$ gives the minimum distance, by which Pareto set approximaiton needs to or can be translated in each dimension in objective space, such that the other approximation is weakly dominated.

\subsection{Measuring diversity}
Diversity metrics help us avoid pareto approximations, that have too similiar solutions. We used two metrics, one taken from NSGAii, the other from Spea2.

\paragraph{Crowding distance metric}
This is the metric that NSGAii uses to ensure diversity of next generation.

\paragraph{Spacing metric}
This is the metric that SPEA2 uses to ensure diversity of the result-archive. 

\subsection{Population size and Number of generations}
These two parameters set the boundaries on minimal and maximal number of evaluations. Lets mark the size of population $N$ and number of generations $G$ Because the whole initial population needs to be evaluated, number of evaluations will at least the size of populations.
Because at each new generation, at most $N$ new individuals are generated, therefore in one optimization run, at most $N \times G$ evaluations.

\subsection{Mutation and Crossover}
Probability of mutation, or crossover directly influence number of generated individuals in each generations.


\chapter{Experiments}
To test our framework we have decided to optimize a simple intrusion detection system on a model wireless sensor network.

\section{Wireless sensor network}
Implementation of WSN was reused from previous research (Stetsko 2012), with just a slight modifications to decouple it from previous optimisation framework. Nodes are simulated with MiXiM framework, with test application on each of them, that sends a packet containing arbitrary information through the network to a base station. Network has a static routing tree based on TODO algorithm. 

We have marked some of the nodes as "droppers", so that instead of forwarding all of the packets through the network, they drop certain percentage. In our case, dropping ratio was set to $0.5$.

\section{Intrusion Detetection System}
Our nodes implement a simple IDS capable of detecting nodes that seem to be intentionaly dropping packets.
If we look at the wireless network stack of our node, IDS is part of the medium access controll sublayer.
There it can eavesdrop on neighbouring nodes and check whether they behave accordingly.

For each neighbour it updates 
\begin{itemize}
\item number of packets recieved 
\item number of packets forwarded.
\end{itemize}

We wanted to use IDS that is simple, but highly configurable. 
We have these four parameters to optimize:
\begin{enumerate}
\item number of monitored nodes
\item size of buffer for eavesdropped packets
\item treshold for minimal number of recieved packets for a node to be considered malicious
\item treshold for ratio between forwarded and recieved packets for a node to be considered to be malicious
\end{enumerate}
At the end of simulation, based on a preset tresholds and ratio between forwarded and reieved packed it decides whether node is malicious or benign. More specificialy, the set of malicious nodes is:

$$ M = \{n \in N|r_n \geq r_{min} \land \frac{f_n}{r_n} \geq d_{max} \} $$
And the set of benign nodes is:

$$B = \{n \in N|r_n < r_{min} \lor \frac{f_n}{r_n} < d_{max} \} $$

You can see, that both sets are disjoint, and their union is the set of neighbours $N$

\section{Setup}
We want to use our framework to optimize IDS in this boundaries:

Because this experiment is aimed primarily on showcasing the functionality of our framework, 
we have attempted meta optimization with the help of sample pareto-front, as well as TODO.
We have used exhaustive search to sample the Pareto front and determine the TODO,
therefore we could check, whether our assumptions hold. 

Our mutation function had three parameters,probability that individual will be mutated $pMut$ and relative $\delta$ neighbourhood around the old value, that the new value will come from:

TODO CODE,IMAGE

Our cross-over function, with parameters $pCross$ (probability that two individuals will be crossed over) and $crossProb$ (probability that params of the two individuials will be swapped):

Calibration was first done on fixed number of generations and population size, with optimizing for values $pMut$, $\delta$, and the type of algorithm, without using cross-over at all ($pCross=0$).


\section{Results}


\bibliographystyle{plain} % sets plain bibliography style
\bibliography{bib-db}
% BibTeX database file
\end{document}

